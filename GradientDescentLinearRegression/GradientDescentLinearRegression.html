<h1 id="gradient-descent-and-simple-linear-regression-logic-and-python-tutorial">Gradient Descent and Simple Linear Regression: Logic and Python tutorial</h1>
<p><em>Sophie Marchand - May 2020</em></p>
<h2 id="shortcut">Shortcut</h2>
<p>Gradient descent is an optimization algorithm employed to find the parameters <span class="math inline"><strong>v</strong></span> of a function <span class="math inline"><em>f</em></span> minimizing a cost function <span class="math inline"><em>C</em></span>. Its iterative procedure is as follow: 1. Initialize parameters to random small values 2. Calculate the cost function <span class="math inline"><em>C</em></span> over the all training data 3. Compute the update of the parameters <span class="math inline"><strong>v</strong></span> with <span class="math inline"><strong>v</strong> − <em>η</em>∇<em>C</em>(<strong>v</strong>)</span> with <span class="math inline"><em>η</em></span> the learning rate 4. Repeat the steps 2 and 3 until reaching “good” enough parameters</p>
<p>This procedure is for one pass of the batch gradient descent. For the stochastic one, step 1 includes a randomization of the training data and step 2 is performed over one instance selected according to the algorithm iteration. Then, the steps 2 and 3 are performed for each randomized training input.</p>
<p>[Output of the tutorial][Figure_GradientDescentLinearRegression.png]</p>
<h2 id="logic-details">Logic details</h2>
<p>We show here the logic behind the gradient descent applied to a simple linear regression problem. The objective of this regression is to find the optimal slop <span class="math inline"><em>b</em></span> and intercept <span class="math inline"><em>a</em></span> which verify <span class="math inline"><em>y</em> = <em>a</em> + <em>b</em> × <em>x</em></span> while minimizing the prediction error for a set of <span class="math inline"><em>n</em></span> points <span class="math inline">(<em>x</em>, <em>y</em>)</span>. In this work, the error function chosen is the Sum of Squared Residuals (<span class="math inline"><em>S</em><em>S</em><em>R</em></span>) defined by the following equation where <span class="math inline"><strong>v</strong></span> is the vector of coefficients <span class="math inline">$\begin{pmatrix}a \\ b \end{pmatrix}$</span> and <span class="math inline"><em>y</em><sub><em>p</em><em>r</em><em>e</em><em>d</em></sub></span> the predicted output variable.</p>
<p><br /><span class="math display">$$SSR(\mathbf{v}) = \frac{1}{2n}\sum_{i=1}^{n}(y_{pred}(\mathbf{v}, i)-y_{i})^{2}$$</span><br /></p>
<p>Using the Taylor series expansion on <span class="math inline"><em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span>, we obtain the expression:</p>
<p><br /><span class="math display"><em>S</em><em>S</em><em>R</em>(<strong>v</strong> + <strong>ϵ</strong>) ≈ <em>S</em><em>R</em><em>S</em>(<strong>v</strong>) + <strong>ϵ</strong><sup><em>T</em></sup>∇<em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span><br /></p>
<p>Then, replacing <span class="math inline"><strong>ϵ</strong></span> by <span class="math inline"> − <em>η</em>∇<em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span> with <span class="math inline"><em>η</em></span> a small positive value called learning rate, we have the relation:</p>
<p><br /><span class="math display"><em>S</em><em>S</em><em>R</em>(<strong>v</strong> − <em>η</em>∇<em>S</em><em>S</em><em>R</em>(<strong>v</strong>)) ≈ <em>S</em><em>S</em><em>R</em>(<strong>v</strong>) − <em>η</em>∇<em>S</em><em>S</em><em>R</em>(<strong>v</strong>)<sup>2</sup> ≤ <em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span><br /></p>
<p>We deduce from the previous expression that updating <span class="math inline"><strong>v</strong></span> by <span class="math inline"><strong>v</strong> − <em>η</em>∇<em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span> may reduce the value of <span class="math inline"><em>S</em><em>S</em><em>R</em>(<strong>v</strong>)</span>. This is the logic adopted by the gradient descent method consisting in the following steps:</p>
<ol type="1">
<li>Initiate the values of <span class="math inline"><strong>v</strong></span> to zero or small random values</li>
</ol>
<ul>
<li>[Stochastic gradient descent] Randomized the training data order, giving the order array <span class="math inline"><em>r</em></span></li>
</ul>
<ol start="2" type="1">
<li>Compute the prediction error <span class="math inline">(<em>y</em><sub><em>p</em><em>r</em><em>e</em><em>d</em></sub>(<strong>v</strong>, <em>i</em>) − <em>y</em><sub><em>i</em></sub>)</span> for:</li>
</ol>
<ul>
<li>[Batch gradient descent] all training data <span class="math inline"><strong>i</strong></span> before calculating the update</li>
<li>[Stochastic gradient descent] each training data instance <span class="math inline"><strong>i</strong></span> and calculate the update immediately</li>
</ul>
<ol start="3" type="1">
<li>Compute the update of <span class="math inline"><strong>v</strong></span> with:</li>
</ol>
<ul>
<li>[Batch gradient descent] <span class="math inline"><em>i</em> ⊂ [1, <em>n</em>]</span></li>
</ul>
<p><br /><span class="math display">$$a := a - \eta\frac{\partial SSR(\mathbf{v}, i)}{\partial a} = a - \frac{\eta}{n}\sum_{i=1}^{n}(y_{pred}(\mathbf{v}, i)-y_{i})$$</span><br /> <br /><span class="math display">$$b := b - \eta\frac{\partial SSR(\mathbf{v}, i)}{\partial b} = b - \frac{\eta}{n}\sum_{i=1}^{n}(y_{pred}(\mathbf{v}, i)-y_{i})x_{i}$$</span><br /></p>
<ul>
<li>[Stochastic gradient descent] <span class="math inline"><em>i</em> = <em>r</em>[<em>j</em>]</span> with j the iteration of the gradient descent <br /><span class="math display">$$ a := a - \eta\frac{\partial SSR(\mathbf{v}, r[j])}{\partial a} = a - \eta(y_{pred}(\mathbf{v}, r[j])-y_{r[j]})$$</span><br /> <br /><span class="math display">$$b := b - \eta\frac{\partial SSR(\mathbf{v}, r[j])}{\partial b} = b - \eta(y_{pred}(\mathbf{v}, r[j])-y_{r[j]})x_{r[j]}$$</span><br /></li>
</ul>
<ol start="4" type="1">
<li>Repeat the steps 2 and 3 until reaching “good” enough coefficients. The performance threshold <span class="math inline"><em>t</em><em>h</em><sub><em>p</em></sub></span> could be defined as value on the Root Mean Square Error (<span class="math inline"><em>R</em><em>M</em><em>S</em><em>E</em></span>) such that we should verify: <br /><span class="math display">$$RMSE = \sqrt{\frac{\sum_{i=0}^{n}(y_{i}^{pred} - y_{i})^{2}}{n}} &lt; th_{p}$$</span><br /></li>
</ol>
<p>Remarks: the stochastic gradient descent is preferred to the batch one for large datasets. To note also that stochastic gradient descent will require a small number of passes through the dataset to reach “good” enough coefficients typically between 1-to-10 passes.</p>
<h2 id="python-tutorial">Python tutorial</h2>
<p>The code source displayed below can be found on <a href="https://github.com/SophMarch/Tutorials">GitHub</a> under Python_GradientDescentLinearRegression.py</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">Tutorial Gradient Descent for Linear Regression</span></span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">Author: Sophie Marchand</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co"># Function initialization parameters, update parameters and compute error</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="kw">def</span> init_parameters():</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="cf">return</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="kw">def</span> randomized_training_data(x, y):</span>
<span id="cb1-16"><a href="#cb1-16"></a>    index_array <span class="op">=</span> np.arange(<span class="bu">len</span>(x))</span>
<span id="cb1-17"><a href="#cb1-17"></a>    np.random.shuffle(index_array)</span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="cf">return</span> x[index_array], y[index_array]</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="kw">def</span> update_parameters_batch(a, b, x, y, learning_rate):</span>
<span id="cb1-22"><a href="#cb1-22"></a>    a_update <span class="op">=</span> a <span class="op">-</span> learning_rate <span class="op">*</span> <span class="bu">sum</span>((a <span class="op">+</span> b <span class="op">*</span> x) <span class="op">-</span> y) <span class="op">/</span> <span class="bu">len</span>(x)</span>
<span id="cb1-23"><a href="#cb1-23"></a>    b_update <span class="op">=</span> b <span class="op">-</span> learning_rate <span class="op">*</span> <span class="bu">sum</span>(((a <span class="op">+</span> b <span class="op">*</span> x) <span class="op">-</span> y) <span class="op">*</span> x) <span class="op">/</span> <span class="bu">len</span>(x)</span>
<span id="cb1-24"><a href="#cb1-24"></a>    <span class="cf">return</span> a_update, b_update</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="kw">def</span> update_parameters_stochastic(a, b, x, y, learning_rate, iteration):</span>
<span id="cb1-28"><a href="#cb1-28"></a>    x_it <span class="op">=</span> x[iteration]</span>
<span id="cb1-29"><a href="#cb1-29"></a>    y_it <span class="op">=</span> y[iteration]</span>
<span id="cb1-30"><a href="#cb1-30"></a>    a_update <span class="op">=</span> a <span class="op">-</span> learning_rate <span class="op">*</span> ((a <span class="op">+</span> b <span class="op">*</span> x_it) <span class="op">-</span> y_it)</span>
<span id="cb1-31"><a href="#cb1-31"></a>    b_update <span class="op">=</span> b <span class="op">-</span> learning_rate <span class="op">*</span> ((a <span class="op">+</span> b <span class="op">*</span> x_it) <span class="op">-</span> y_it) <span class="op">*</span> x_it</span>
<span id="cb1-32"><a href="#cb1-32"></a>    <span class="cf">return</span> a_update, b_update</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="kw">def</span> compute_error_rmse(a, b, x, y):</span>
<span id="cb1-36"><a href="#cb1-36"></a>    <span class="cf">return</span> np.sqrt(<span class="bu">sum</span>(np.square((a <span class="op">+</span> b <span class="op">*</span> x) <span class="op">-</span> y)) <span class="op">/</span> <span class="bu">len</span>(x))</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a><span class="co"># Create a set of points (x,y) and set a learning rate</span></span>
<span id="cb1-40"><a href="#cb1-40"></a>x <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>])</span>
<span id="cb1-41"><a href="#cb1-41"></a>y <span class="op">=</span> np.array([<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>])</span>
<span id="cb1-42"><a href="#cb1-42"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb1-43"><a href="#cb1-43"></a>number_batch <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-44"><a href="#cb1-44"></a>rmse_batch <span class="op">=</span> []</span>
<span id="cb1-45"><a href="#cb1-45"></a>rmse_stochastic <span class="op">=</span> []</span>
<span id="cb1-46"><a href="#cb1-46"></a></span>
<span id="cb1-47"><a href="#cb1-47"></a><span class="co"># Workflow batch gradient descent to estimate (a,b) parameters of y = a + b*x</span></span>
<span id="cb1-48"><a href="#cb1-48"></a>a, b <span class="op">=</span> init_parameters()</span>
<span id="cb1-49"><a href="#cb1-49"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(number_batch):</span>
<span id="cb1-50"><a href="#cb1-50"></a>    a_update, b_update <span class="op">=</span> update_parameters_batch(a, b, x, y, learning_rate)</span>
<span id="cb1-51"><a href="#cb1-51"></a>    rmse_batch.append(compute_error_rmse(a_update, b_update, x, y))</span>
<span id="cb1-52"><a href="#cb1-52"></a>    a, b <span class="op">=</span> a_update, b_update</span>
<span id="cb1-53"><a href="#cb1-53"></a>a_batch, b_batch <span class="op">=</span> a, b</span>
<span id="cb1-54"><a href="#cb1-54"></a></span>
<span id="cb1-55"><a href="#cb1-55"></a><span class="co"># Workflow stochastic gradient descent to estimate (a,b) parameters of y = a + b*x</span></span>
<span id="cb1-56"><a href="#cb1-56"></a>a, b <span class="op">=</span> init_parameters()</span>
<span id="cb1-57"><a href="#cb1-57"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(number_batch):</span>
<span id="cb1-58"><a href="#cb1-58"></a>    x_rand, y_rand <span class="op">=</span> randomized_training_data(x, y)</span>
<span id="cb1-59"><a href="#cb1-59"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb1-60"><a href="#cb1-60"></a>        a_update, b_update <span class="op">=</span> update_parameters_stochastic(a, b, x_rand, y_rand, learning_rate, iteration)</span>
<span id="cb1-61"><a href="#cb1-61"></a>        a, b <span class="op">=</span> a_update, b_update</span>
<span id="cb1-62"><a href="#cb1-62"></a>    rmse_stochastic.append(compute_error_rmse(a, b, x, y))</span>
<span id="cb1-63"><a href="#cb1-63"></a>a_stochastic, b_stochastic <span class="op">=</span> a, b</span>
<span id="cb1-64"><a href="#cb1-64"></a></span>
<span id="cb1-65"><a href="#cb1-65"></a><span class="co"># Print results</span></span>
<span id="cb1-66"><a href="#cb1-66"></a>y_batch <span class="op">=</span> a_batch <span class="op">+</span> b_batch <span class="op">*</span> x</span>
<span id="cb1-67"><a href="#cb1-67"></a>y_stochastic <span class="op">=</span> a_stochastic <span class="op">+</span> b_stochastic <span class="op">*</span> x</span>
<span id="cb1-68"><a href="#cb1-68"></a></span>
<span id="cb1-69"><a href="#cb1-69"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-70"><a href="#cb1-70"></a>fig.suptitle(<span class="st">&#39;Gradient descent for linear regression with two methods: batch &amp; stochastic for &#39;</span></span>
<span id="cb1-71"><a href="#cb1-71"></a>             <span class="op">+</span> <span class="bu">str</span>(number_batch) <span class="op">+</span> <span class="st">&#39; passes and a learning rate of &#39;</span> <span class="op">+</span> <span class="bu">str</span>(learning_rate), fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-72"><a href="#cb1-72"></a>ax1.plot(x, y, <span class="st">&#39;bo&#39;</span>, label<span class="op">=</span><span class="st">&#39;Initial data&#39;</span>)</span>
<span id="cb1-73"><a href="#cb1-73"></a>ax1.plot(x, y_batch, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, marker<span class="op">=</span><span class="st">&#39;+&#39;</span>,</span>
<span id="cb1-74"><a href="#cb1-74"></a>         color<span class="op">=</span><span class="st">&#39;r&#39;</span>, label<span class="op">=</span><span class="st">&#39;Batch gradient descent&#39;</span>)</span>
<span id="cb1-75"><a href="#cb1-75"></a>ax1.plot(x, y_stochastic, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, marker<span class="op">=</span><span class="st">&#39;x&#39;</span>,</span>
<span id="cb1-76"><a href="#cb1-76"></a>         color<span class="op">=</span><span class="st">&#39;g&#39;</span>, label<span class="op">=</span><span class="st">&#39;Stochastic gradient descent&#39;</span>)</span>
<span id="cb1-77"><a href="#cb1-77"></a>ax1.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">&#39;x&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;y&#39;</span>,</span>
<span id="cb1-78"><a href="#cb1-78"></a>        title<span class="op">=</span><span class="st">&#39;Linear regression results&#39;</span>)</span>
<span id="cb1-79"><a href="#cb1-79"></a>ax1.legend()</span>
<span id="cb1-80"><a href="#cb1-80"></a>ax1.text(<span class="fl">4.2</span>, <span class="fl">5.4</span>, <span class="vs">r&#39;$a=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(a_batch)[:<span class="dv">4</span>] <span class="op">+</span></span>
<span id="cb1-81"><a href="#cb1-81"></a>         <span class="st">&#39;, $b=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(b_batch)[:<span class="dv">4</span>] <span class="op">+</span> <span class="st">&#39; and $rmse=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(rmse_batch[<span class="op">-</span><span class="dv">1</span>])[:<span class="dv">4</span>], fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">&#39;r&#39;</span>)</span>
<span id="cb1-82"><a href="#cb1-82"></a>ax1.text(<span class="fl">4.2</span>, <span class="fl">4.8</span>, <span class="vs">r&#39;$a=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(a_stochastic)[:<span class="dv">4</span>] <span class="op">+</span> <span class="st">&#39;, $b=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(b_stochastic)[:<span class="dv">4</span>] <span class="op">+</span></span>
<span id="cb1-83"><a href="#cb1-83"></a>         <span class="st">&#39; and $rmse=$&#39;</span> <span class="op">+</span> <span class="bu">str</span>(rmse_stochastic[<span class="op">-</span><span class="dv">1</span>])[:<span class="dv">4</span>], fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">&#39;g&#39;</span>)</span>
<span id="cb1-84"><a href="#cb1-84"></a>ax2.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(rmse_batch)<span class="op">+</span><span class="dv">1</span>), rmse_batch, <span class="st">&#39;r+-&#39;</span>, label<span class="op">=</span><span class="st">&#39;Error batch gradient descent&#39;</span>)</span>
<span id="cb1-85"><a href="#cb1-85"></a>ax2.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(rmse_stochastic)<span class="op">+</span><span class="dv">1</span>), rmse_stochastic, <span class="st">&#39;gx-&#39;</span>, label<span class="op">=</span><span class="st">&#39;Error stochastic gradient descent&#39;</span>)</span>
<span id="cb1-86"><a href="#cb1-86"></a>ax2.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">&#39;number pass&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;RMSE&#39;</span>,</span>
<span id="cb1-87"><a href="#cb1-87"></a>        title<span class="op">=</span><span class="st">&#39;Error prediction for each pass&#39;</span>)</span>
<span id="cb1-88"><a href="#cb1-88"></a>ax2.legend()</span>
<span id="cb1-89"><a href="#cb1-89"></a>plt.show()</span></code></pre></div>
